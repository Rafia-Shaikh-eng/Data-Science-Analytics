{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rafia-Shaikh-eng/Data-Science-Analytics/blob/main/1_csv_exercise_ETL_Extract.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAalPJ7FpzPs"
      },
      "source": [
        "# Goal of the ETL Lesson\n",
        "\n",
        "The main goal of this ETL pipelines lesson is to take the [World Bank Project data set](https://datacatalog.worldbank.org/dataset/world-bank-projects-operations) and merge this data with the [World Bank indicator data](https://data.worldbank.org/indicator/SP.POP.TOTL). Then you'll load the merged data into a database.\n",
        "\n",
        "In the process, you'll need to transform these data sets in different ways. And finally, you'll write a single Python module that reads in these date sets, transforms them, and loads the results into the database all in one step.\n",
        "\n",
        "# Extracting data from a csv file\n",
        "\n",
        "The first step in an ETL pipeline is extraction. Data comes in all types of different formats, and you'll practice extracting data from csv files, JSON files, XML files, SQL databases, and the web.\n",
        "\n",
        "In this first exercise, you'll practice extracting data from a CSV file and then navigating through the results. You'll see that extracting data is not always a straight-forward process.\n",
        "\n",
        "This exercise contains a series of coding questions for you to solve. If you get stuck, there is a solution file called 1_csv_exercise_solution.ipynb. You can find this solution file by going to File->Open and then clicking on the file name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy9F5UnRpzP2"
      },
      "source": [
        "# Part 1 projects_data.csv\n",
        "\n",
        "There are two csv files loaded into this workspace:\n",
        "* projects_data.csv\n",
        "* population_data.csv\n",
        "\n",
        "As a first step, try importing the projects data using the pandas [read_csv method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html). The file path is just 'projects_data.csv'. You can see the file if you click on File->Open in the workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZJkEbOJpzP3"
      },
      "outputs": [],
      "source": [
        "# TODO: import the projects_data.csv file using the pandas library\n",
        "# Store the results in the df_projects variable\n",
        "import pandas as pd\n",
        "df_projects = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uO5xz38pzP7"
      },
      "source": [
        "Did you get a DType warning? Read about what this warning is in the [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.errors.DtypeWarning.html).\n",
        "\n",
        "Pandas tries to figure out programatically the data type of each column (integer, float, boolean, string). In this case, pandas could not automatically figure out the data type. That is because some columns have multiple data types. In other words, this data is somewhat messy.\n",
        "\n",
        "You can use the dtype option to specify the data type of each column. Because there are so many columns in this data set, you can set all columns to be strings.\n",
        "\n",
        "Try reading in the data set again using the read_csv() method. This time, also use the option dtype=str so that pandas treats everything like a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEHXdoFUpzP8"
      },
      "outputs": [],
      "source": [
        "# TODO: Read in the projects_data.csv file using the read_csv method\n",
        "# and dtype = str option\n",
        "df_projects = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkfMGMrqpzP9"
      },
      "outputs": [],
      "source": [
        "# Run the cell below to see what the data looks like\n",
        "df_projects.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srKiS-CjpzP-"
      },
      "outputs": [],
      "source": [
        "# TODO: count the number of null values in the data set\n",
        "# HINT: use the isnull() and sum() methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbZ7k-W5pzP_"
      },
      "source": [
        "Notice that the number 18248 shows up multiple times. There is also a countryname column with 0 missing values and a Country column with 14045 missing values. This data set has some issues that will need to be solved in the transform part of the pipeline.\n",
        "\n",
        "Next, output the size of the data frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJtncfVbpzQA"
      },
      "outputs": [],
      "source": [
        "# TODO: output the shape of the data frame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHGXYbt-pzQC"
      },
      "source": [
        "There are 18248 rows in this data set. Considering many columns had 18248 NaN values, many columns in the data set are filled completely with NaN values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XXubusapzQD"
      },
      "source": [
        "# Part 2 population_data.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyxAupbApzQE"
      },
      "source": [
        "Next, use the pandas read_csv method to read in the population_data.csv file. The path to this file is \"population_data.csv\". When you try to read in this data set using pandas, you'll get an error because there is something wrong with the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tn8kM2CHpzQF"
      },
      "outputs": [],
      "source": [
        "# TODO: read in the population_data.csv file\n",
        "# Put the results in a variable called df_population\n",
        "df_population = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3szaNFgpzQF"
      },
      "source": [
        "There is something wrong with this data set. You should see an error that says \"expected 3 fields in line 5, saw 63\". What might have happened? Try printing out the first few lines of the data file to see what the issue might be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UjlinpgpzQG"
      },
      "outputs": [],
      "source": [
        "# TODO: Print out the first 10 lines of the data set, line by line.\n",
        "# HINT: You can't use the read_csv method from pandas\n",
        "# HINT: to do this manually, you'll need to use pure Python\n",
        "# HINT: the open(), readline(), and close() methods should be helpful\n",
        "# HINT: Use a for loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxKjm2hcpzQH"
      },
      "source": [
        "The first four lines in the file are not properly formatted and don't contain data. Next, read in the data using the read_csv method. But this time, use the skiprows option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWz9JFTGpzQH"
      },
      "outputs": [],
      "source": [
        "# TODO: read in population data skipping the first four rows\n",
        "# Put the results in a variable called df_population\n",
        "\n",
        "df_population = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVzWtZnVpzQI"
      },
      "outputs": [],
      "source": [
        "# Run this cell to see what the data looks like\n",
        "df_population.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlTNZGHKpzQJ"
      },
      "source": [
        "Make sure to scroll over to see what the last column looks like. That last column, called 'Unnamed: 62', doesn't look very useful and is filled with NaN values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DLzZ7h-pzQK"
      },
      "outputs": [],
      "source": [
        "# TODO: Count the number of null values in each column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9QVZzMepzQK"
      },
      "source": [
        "It looks like every year column has at least one NaN value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFBigncXpzQL"
      },
      "outputs": [],
      "source": [
        "# TODO: Sum the null values by column\n",
        "# HINT: In the sum method, use axis=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE6vXHcCpzQL"
      },
      "source": [
        "And it looks like almost every row has only one null value. That is probably from the 'Unnamed: 62' column that doesn't have any relevant information in it. Next, drop the 'Unnamed: 62' column from the data frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYZBmFA9pzQM"
      },
      "outputs": [],
      "source": [
        "# TODO: drop the 'Unnamed: 62' column from the data frame,\n",
        "# and save the results in the df_population variable\n",
        "\n",
        "df_population = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BfPMh7YpzQM"
      },
      "outputs": [],
      "source": [
        "# TODO: Now output any row that contains a null value.\n",
        "# HINT: Use the isnull() and any() methods. You'll want to use axis=1\n",
        "# in the any method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMhzGtOxpzQN"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "This population data doesn't look too bad. Only six rows have missing values. In the transformation part of the lesson, you'll have to deal with these missing values somehow.\n",
        "\n",
        "If you would like to see the solution file for this exercise, go to File->Open and click on 1_csv_exercise_solution.ipynb.\n",
        "\n",
        "In the next exercise, you'll practice extracting data json and xml files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LnLBRSYpzQN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}